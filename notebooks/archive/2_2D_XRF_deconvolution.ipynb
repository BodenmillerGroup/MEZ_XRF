{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2D XRF deconvolution of pre-processed hdf files\n",
    "## Summary\n",
    "This notebook takes the pre-processed .hdf files produced with the `1_reduced_reshaped_xrf_hdf` notebook and deconvolutes to element channel images using the PyMCA package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from PyMca5.PyMcaPhysics.xrf.XRFBatchFitOutput import OutputBuffer\n",
    "from PyMca5.PyMcaPhysics.xrf.FastXRFLinearFit import FastXRFLinearFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deconvoluted hdf files will be output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\n"
     ]
    }
   ],
   "source": [
    "# Set data directory to work from \n",
    "base_dir = \"C:/Users/MerrickS/OneDrive/Work/2_UZH/Papers/1_MEZ_XRF\"\n",
    "base_dir = pathlib.Path(base_dir)\n",
    "\n",
    "# Specify the input directory where hdf files to process are located\n",
    "hdf_dir = base_dir / 'data' / 'processed' / 'xrf' / '1_reduced_reshaped_hdfs'\n",
    "\n",
    "# Specify the config directory where config files for deconvolution are located\n",
    "cfg_dir = base_dir / 'data' / 'raw' / 'xrf' / 'config'\n",
    "\n",
    "# Make output directory for reshaped hdf files if it does not exist\n",
    "out_dir = base_dir / 'data' / 'processed' / 'xrf' / '2_deconvoluted_hdfs'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('\\n Deconvoluted hdf files will be output to: \\n\\t', base_dir, out_dir) \n",
    "\n",
    "# Gather filepaths for preprocessed hdfs and config files for XRF fitting\n",
    "hdf_filepaths = list(hdf_dir.glob('*.h5'))\n",
    "\n",
    "# Read in scan and scanset metadata\n",
    "df_hdf_config = pd.read_csv(hdf_dir /'preprocessed_hdf_config_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make hdf XRF fit config dictionary \n",
    "hdf_list = df_hdf_config['hdf_file'].tolist()\n",
    "config_list = df_hdf_config['config_file'].tolist()\n",
    "hdf_config_dict = dict(zip(hdf_list, config_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_dataset(hdf_filepath, dataset): \n",
    "    with h5py.File(hdf_filepath, 'r') as hdf:\n",
    "        node = f\"{dataset}\"\n",
    "        try:\n",
    "            hdf[node]\n",
    "            dset = hdf[dataset]\n",
    "        except KeyError:\n",
    "            dset = []\n",
    "            print(f'Could not find hdf dataset for {hdf_filepath}')\n",
    "\n",
    "        dset = dset[:]\n",
    "    return dset\n",
    "\n",
    "def xrf_deconvolute(hdf_file, detector, refit_state=False, weight=0):\n",
    "    # Check hdf file exists\n",
    "    input_hdf_fpath = hdf_dir / f'{hdf_file}.h5'\n",
    "    if input_hdf_fpath.exists() is False:\n",
    "        print(f'{input_hdf_fpath} does not exist')\n",
    "        \n",
    "    # Check detector node exists\n",
    "    node_exists = False\n",
    "    with h5py.File(input_hdf_fpath, 'r+') as hdf:  \n",
    "        det_entries = len(hdf[detector][:])\n",
    "        print(det_entries)\n",
    "        if det_entries > 100:\n",
    "            node_exists = True\n",
    "        else:\n",
    "            print(detector, 'empty, no fit performed')\n",
    "\n",
    "         #   node_exists = True\n",
    "          #  print('node exists')\n",
    "    \n",
    "    # Check cfg file exists\n",
    "    config_filepath = cfg_dir / hdf_config_dict[hdf_file]\n",
    "    if config_filepath.exists() is False:\n",
    "        print(f'{config_filepath} does not exist')\n",
    "        \n",
    "    # Make output subdir if does not exist\n",
    "    out_sub_dir = out_dir / 'per_detector_deconvolutions' / hdf_file / detector\n",
    "    out_sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # Get dataset to deconvolute and reshape to 3D array (2D array of spectra) for PyMCA\n",
    "    spectra_stack = hdf_dataset(input_hdf_fpath, detector)\n",
    "    z_dim = len(list(set(hdf_dataset(input_hdf_fpath, 'hrz'))))\n",
    "    y_dim = int(len(list(hdf_dataset(input_hdf_fpath, 'hry')))/z_dim)\n",
    "    spectra_stack_sorted = spectra_stack.reshape(z_dim, y_dim, spectra_stack.shape[-1])  \n",
    "    \n",
    "    # Pymca fit\n",
    "    if node_exists == True:\n",
    "        pymca_object = FastXRFLinearFit()    \n",
    "        \n",
    "        print('spectra config file:', config_filepath.name)\n",
    "\n",
    "        FastXRFLinearFit.setFitConfigurationFile(pymca_object, str(config_filepath))\n",
    "\n",
    "        test_out = OutputBuffer(outputDir=out_sub_dir,\n",
    "                                edf=0,\n",
    "                                tif=True,\n",
    "                                overwrite=True,\n",
    "                                outputRoot=hdf_file, \n",
    "                                fileEntry=hdf_file\n",
    "                                )\n",
    "               \n",
    "        FastXRFLinearFit.fitMultipleSpectra(pymca_object, \n",
    "                                            #x=spectra_stack[1],\n",
    "                                            y=spectra_stack_sorted, \n",
    "                                            weight=weight, \n",
    "                                            refit=refit_state, \n",
    "                                            concentrations=False,\n",
    "                                            outbuffer=test_out)\n",
    "        \n",
    "        # Carry over the fpico normalisation mask\n",
    "        output_hdf_fpath = list(out_sub_dir.glob('*h5'))[0]\n",
    "        \n",
    "        with h5py.File(input_hdf_fpath, 'r') as hdf:\n",
    "            if 'fpico_mask' in hdf:\n",
    "                fpico_mask = hdf['fpico_mask'][:]\n",
    "            else:\n",
    "                fpico_mask = False\n",
    "                \n",
    "        print('output hdf file:', output_hdf_fpath)\n",
    "\n",
    "        if isinstance(fpico_mask, np.ndarray):\n",
    "            with h5py.File(output_hdf_fpath, 'r+') as hdf:\n",
    "                  hdf.create_dataset(name = 'fpico_mask', data = fpico_mask)\n",
    "\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform deconvolutions on both detectors (where present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " appendix_a1_overview_solid_0001\n",
      "219200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 897843200 into shape (176,1246,4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-074fad425705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mhdf_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhdf_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mxrf_deconvolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'falconx_det0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mxrf_deconvolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fluodet_det0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-621e57708df3>\u001b[0m in \u001b[0;36mxrf_deconvolute\u001b[1;34m(hdf_file, detector, refit_state, weight)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mz_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hdf_fpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hrz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0my_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hdf_fpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mspectra_stack_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspectra_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectra_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#     # Pymca fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 897843200 into shape (176,1246,4096)"
     ]
    }
   ],
   "source": [
    "for hdf_file in hdf_list:\n",
    "    print('\\n', hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state=True)\n",
    "    xrf_deconvolute(hdf_file, 'fluodet_det0', refit_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Incorporate step sizes for final channel image hdf files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key acquisition metadata incorporated to .hdf files\n"
     ]
    }
   ],
   "source": [
    "# Incorporate key csv attributes per scan to deconvoluted hdfs for subsequent analysis\n",
    "hdf_img_fpaths = [i for i in out_dir.glob('*/*/*/*.h5')]\n",
    "\n",
    "for hdf_img_fpath in hdf_img_fpaths:\n",
    "    step = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'step_um'].iloc[0]\n",
    "    det_type = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'detector'].iloc[0]\n",
    "    dual_det = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'dual_detector'].iloc[0]\n",
    "    \n",
    "    if dual_det == 1:\n",
    "        dual_det = 'yes'\n",
    "    else:\n",
    "        dual_det = 'no'\n",
    "   \n",
    "    with h5py.File(hdf_img_fpath, 'r+') as hdf:\n",
    "        if 'pixel_um' in hdf:\n",
    "            pass \n",
    "        else:\n",
    "            hdf.create_dataset(name = 'detector', data = step)\n",
    "        if 'detector' in hdf:\n",
    "            pass \n",
    "        else:\n",
    "            hdf.create_dataset(name = 'detector', data = det_type)\n",
    "        if 'dual_detector' in hdf:\n",
    "            pass \n",
    "        else:\n",
    "            hdf.create_dataset(name = 'dual_detector', data = dual_det)\n",
    "\n",
    "print('Key acquisition metadata incorporated to .hdf files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate hdfs to summary hdfs per sample. For scans with dual detectors, element plots will be aggregated, while for single detector scans these will simply be copied over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary hdfs output to:  C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\summary_hdfs\n"
     ]
    }
   ],
   "source": [
    "detector_paths = ['falconx_det0', 'fluodet_det0']\n",
    "out_sum_dir = out_dir / 'summary_hdfs'\n",
    "out_sum_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the falconX detector to summary scan output\n",
    "for hdf_file in df_hdf_config['hdf_file']:\n",
    "    hdf_dir = out_dir / 'per_detector_deconvolutions' / hdf_file \n",
    "    hdf_fpath = list(hdf_dir.glob('*/*.h5'))\n",
    "    \n",
    "    shutil.copyfile(hdf_fpath[0], out_sum_dir / hdf_fpath[0].name)\n",
    "            \n",
    "# For dual detector scans, modify the summary scan datasets by aggregating second detector data for plots\n",
    "for hdf_file in df_hdf_config.loc[df_hdf_config['dual_detector'] == 1, 'hdf_file']:\n",
    "    hdf_dir = out_dir / 'per_detector_deconvolutions' / hdf_file \n",
    "    hdf_fpath = list(hdf_dir.glob('*/*.h5'))\n",
    "    \n",
    "    hdf_base = h5py.File(hdf_fpath[0], 'r+')\n",
    "    plots = list(hdf_base[f'{hdf_file}/plotselect'].keys())\n",
    "    \n",
    "    for plot in plots:\n",
    "        dset = hdf_base[f'{hdf_file}/plotselect/{plot}'][:]\n",
    "        \n",
    "        with h5py.File(hdf_fpath[1], 'r+') as hdf_add:\n",
    "            dset_add = hdf_add[f'{hdf_file}/plotselect/{plot}'][:]\n",
    "            \n",
    "        dset_mean = np.mean([dset, dset_add], axis=0)\n",
    "        \n",
    "        hdf_mod_path = out_sum_dir / hdf_fpath[0].name\n",
    "        #print(hdf_mod_path.exists())\n",
    "        with h5py.File(hdf_mod_path, 'r+') as hdf_mod:\n",
    "            hdf_mod[f'{hdf_file}/plotselect/{plot}'][...] = dset_mean\n",
    "        \n",
    "    #print(dset_mean.shape) \n",
    "    hdf_base.close()\n",
    "    \n",
    "print('Summary hdfs output to: ', out_sum_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\\n\\n# Refits of silicon drift detector cell pellets\\nhdf_sublist = ['001_002_stitch', '001_004_stitch',  'sample001_0001', 'sample001_0003']\\n\\nfor hdf_file in hdf_sublist:\\n    print(hdf_file)\\n    xrf_deconvolute(hdf_file, 'falconx_det0')\\n    xrf_deconvolute(hdf_file, 'fluodet_det0')\\n    \\nprint('deconvolutions complete')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\n",
    "\n",
    "# Refits of silicon drift detector cell pellets\n",
    "hdf_sublist = ['001_002_stitch', '001_004_stitch',  'sample001_0001', 'sample001_0003']\n",
    "\n",
    "for hdf_file in hdf_sublist:\n",
    "    print(hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0')\n",
    "    xrf_deconvolute(hdf_file, 'fluodet_det0')\n",
    "    \n",
    "print('deconvolutions complete')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\\n\\n# Refits of GeCMOS detector breast cancers\\nhdf_sublist = ['sample304_0001', 'sample304_0002', 'sample304_b_0001', 'sample304_b_0002', 'sample304_b_0003', 'sample304_b_0004']\\n\\nfor hdf_file in hdf_sublist:\\n    print(hdf_file)\\n    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state = True, weight=1)\\n    \\nprint('deconvolutions complete')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\n",
    "\n",
    "# Refits of GeCMOS detector breast cancers\n",
    "hdf_sublist = ['sample304_0001', 'sample304_0002', 'sample304_b_0001', 'sample304_b_0002', 'sample304_b_0003', 'sample304_b_0004']\n",
    "\n",
    "for hdf_file in hdf_sublist:\n",
    "    print(hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state = True, weight=1)\n",
    "    \n",
    "print('deconvolutions complete')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
