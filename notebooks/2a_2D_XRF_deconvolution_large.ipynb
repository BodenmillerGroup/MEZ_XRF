{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2D XRF deconvolution of pre-processed hdf files\n",
    "## Summary\n",
    "This notebook takes the pre-processed .hdf files produced with the `1_reduced_reshaped_xrf_hdf` notebook and deconvolutes to element channel images using the PyMCA package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PyMca5.PyMcaPhysics.xrf.XRFBatchFitOutput import OutputBuffer\n",
    "from PyMca5.PyMcaPhysics.xrf.FastXRFLinearFit import FastXRFLinearFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deconvoluted hdf files will be output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\n"
     ]
    }
   ],
   "source": [
    "# Set data directory to work from \n",
    "base_dir = \"C:/Users/MerrickS/OneDrive/Work/2_UZH/Papers/1_MEZ_XRF\"\n",
    "base_dir = pathlib.Path(base_dir)\n",
    "\n",
    "# Specify the input directory where hdf files to process are located\n",
    "hdf_dir = base_dir / 'data' / 'processed' / 'xrf' / '1_reduced_reshaped_hdfs'\n",
    "\n",
    "# Specify the config directory where config files for deconvolution are located\n",
    "cfg_dir = base_dir / 'data' / 'raw' / 'xrf' / 'config'\n",
    "\n",
    "# Make output directory for reshaped hdf files if it does not exist\n",
    "out_dir = base_dir / 'data' / 'processed' / 'xrf' / '2_deconvoluted_hdfs'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('\\n Deconvoluted hdf files will be output to: \\n\\t', base_dir, out_dir) \n",
    "\n",
    "# Gather filepaths for preprocessed hdfs and config files for XRF fitting\n",
    "hdf_filepaths = list(hdf_dir.glob('*.h5'))\n",
    "\n",
    "# Read in scan and scanset metadata\n",
    "df_hdf_config = pd.read_csv(hdf_dir /'preprocessed_hdf_config_files2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make hdf XRF fit config dictionary \n",
    "hdf_list = df_hdf_config['hdf_file'].tolist()\n",
    "config_list = df_hdf_config['config_file'].tolist()\n",
    "hdf_config_dict = dict(zip(hdf_list, config_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_dataset(hdf_filepath, dataset): \n",
    "    with h5py.File(hdf_filepath, 'r') as hdf:\n",
    "        node = f\"{dataset}\"\n",
    "        try:            \n",
    "            hdf[node]\n",
    "            dset = hdf[dataset]\n",
    "        except KeyError:\n",
    "            dset = []\n",
    "            print(f'Could not find hdf dataset for {hdf_filepath}')\n",
    "\n",
    "        dset = dset[:]\n",
    "    return dset\n",
    "\n",
    "def hdf_large_dataset(hdf_filepath, dataset, dims): \n",
    "    with h5py.File(hdf_filepath, 'r') as hdf:\n",
    "        node = f\"{dataset}\"\n",
    "        try:            \n",
    "            hdf[node]\n",
    "            dset = hdf[dataset]\n",
    "        except KeyError:\n",
    "            dset = []\n",
    "            print(f'Could not find hdf dataset for {hdf_filepath}')\n",
    "\n",
    "        dset = dset[dims[0]:dims[1],:]\n",
    "    return dset\n",
    "\n",
    "def xrf_deconvolute(hdf_file, detector, refit_state=False, weight=0):\n",
    "    # Check hdf file exists\n",
    "    input_hdf_fpath = hdf_dir / f'{hdf_file}.h5'\n",
    "    if input_hdf_fpath.exists() is False:\n",
    "        print(f'{input_hdf_fpath} does not exist')\n",
    "        \n",
    "    # Check detector node exists\n",
    "    node_exists = False\n",
    "    with h5py.File(input_hdf_fpath, 'r+') as hdf:  \n",
    "        det_entries = hdf[detector].shape[0]\n",
    "        print(det_entries)\n",
    "        if det_entries > 100:\n",
    "            node_exists = True\n",
    "        else:\n",
    "            print(detector, 'empty, no fit performed')\n",
    "    \n",
    "    # Check cfg file exists\n",
    "    config_filepath = cfg_dir / hdf_config_dict[hdf_file]\n",
    "    if config_filepath.exists() is False:\n",
    "        print(f'{config_filepath} does not exist')\n",
    "        \n",
    "    # Make output subdir if does not exist\n",
    "    out_sub_dir = out_dir / 'per_detector_deconvolutions' / hdf_file / detector\n",
    "    out_sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # Get dataset to deconvolute and reshape to 3D array (2D array of spectra) for PyMCA\n",
    "    # Establish if need to chunk data\n",
    "    if det_entries > 2000000:\n",
    "        z_dim = len(list(set(hdf_dataset(input_hdf_fpath, 'hrz'))))\n",
    "        y_dim = int(len(list(hdf_dataset(input_hdf_fpath, 'hry')))/z_dim)\n",
    "        print('large dataset', 'z', z_dim, 'y', y_dim)\n",
    "        \n",
    "        # Find chunk size            \n",
    "        chunks = 10\n",
    "        if z_dim % chunks == 0:\n",
    "            print(\"Chunks:\", chunks)\n",
    "        else:\n",
    "            print(\"Chunk error, can't find equal division of data\")\n",
    "            \n",
    "        chunk_size = int(det_entries / chunks)\n",
    "        chunk_dims = [[i*chunk_size,(i+1)*chunk_size] for i in range(chunks)]\n",
    "        print(chunk_dims)\n",
    "        \n",
    "    else: \n",
    "        z_dim = len(list(set(hdf_dataset(input_hdf_fpath, 'hrz'))))\n",
    "        y_dim = int(len(list(hdf_dataset(input_hdf_fpath, 'hry')))/z_dim)\n",
    "        print('small dataset', 'z', z_dim, 'y', y_dim)\n",
    "        chunks = 0\n",
    "        \n",
    "    # Deconvolution for chunked data\n",
    "    if chunks != 0:\n",
    "        for chunk in range(chunks):\n",
    "            out_sub_sub_dir = out_sub_dir / f'chunk_{chunk}'\n",
    "            out_sub_sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Deconvoluting spectral chunk:\", chunk_dims[chunk])                    \n",
    "            spectra_stack = hdf_large_dataset(input_hdf_fpath, detector, chunk_dims[chunk])\n",
    "            print(\"spectra shape\", spectra_stack.shape)\n",
    "            spectra_stack = spectra_stack.reshape(int(z_dim/chunks), y_dim, spectra_stack.shape[-1])         \n",
    "            print(\"reshaped spectra shape\", spectra_stack.shape)        \n",
    "    \n",
    "            # Pymca fit\n",
    "            if node_exists == True:\n",
    "                pymca_object = FastXRFLinearFit()    \n",
    "\n",
    "                print('spectra config file:', config_filepath.name)\n",
    "\n",
    "                FastXRFLinearFit.setFitConfigurationFile(pymca_object, str(config_filepath))\n",
    "\n",
    "                test_out = OutputBuffer(outputDir=out_sub_sub_dir,\n",
    "                                        edf=0,\n",
    "                                        tif=False,\n",
    "                                        overwrite=True,\n",
    "                                        outputRoot=hdf_file, \n",
    "                                        fileEntry=hdf_file\n",
    "                                        )\n",
    "\n",
    "                FastXRFLinearFit.fitMultipleSpectra(pymca_object, \n",
    "                                                    y=spectra_stack, \n",
    "                                                    weight=weight, \n",
    "                                                    refit=refit_state, \n",
    "                                                    concentrations=False,\n",
    "                                                    outbuffer=test_out)\n",
    "                \n",
    "                # Extract tifs from hdf channels\n",
    "                output_hdf_fpath = list(out_sub_sub_dir.glob('*h5'))[0]\n",
    "                with h5py.File(output_hdf_fpath, 'r+') as hdf:\n",
    "                    node = f\"{output_hdf_fpath.stem}/plotselect\"\n",
    "\n",
    "                    image_dir = output_hdf_fpath.parent / \"IMAGES\"\n",
    "                    image_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    for img in hdf[node].keys():\n",
    "                        subnode = f\"{node}/{img}\"\n",
    "                        dset = hdf[subnode][...]\n",
    "                        outpath = str(image_dir / f'{img}.tiff')\n",
    "                        cv2.imwrite(outpath, dset)\n",
    "\n",
    "                # Carry over the fpico normalisation mask is exists\n",
    "                output_hdf_fpath = list(out_sub_sub_dir.glob('*h5'))[0]\n",
    "\n",
    "                with h5py.File(input_hdf_fpath, 'r') as hdf:\n",
    "                    if 'fpico_mask' in hdf:\n",
    "                        fpico_mask = hdf['fpico_mask'][:]\n",
    "                    else:\n",
    "                        fpico_mask = False\n",
    "\n",
    "                print('output hdf file:', output_hdf_fpath)\n",
    "\n",
    "                if isinstance(fpico_mask, np.ndarray):\n",
    "                    with h5py.File(output_hdf_fpath, 'r+') as hdf:\n",
    "                          hdf.create_dataset(name = 'fpico_mask', data = fpico_mask)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    # Deconvolution for non-chunked data\n",
    "    if chunks == 0:\n",
    "        spectra_stack = hdf_dataset(input_hdf_fpath, detector)\n",
    "        print(\"spectra shape\", spectra_stack.shape)\n",
    "        spectra_stack = spectra_stack.reshape(int(z_dim), y_dim, spectra_stack.shape[-1])         \n",
    "        print(\"reshaped spectra shape\", spectra_stack.shape)        \n",
    "\n",
    "        # Pymca fit\n",
    "        if node_exists == True:\n",
    "            pymca_object = FastXRFLinearFit()    \n",
    "\n",
    "            print('spectra config file:', config_filepath.name)\n",
    "\n",
    "            FastXRFLinearFit.setFitConfigurationFile(pymca_object, str(config_filepath))\n",
    "\n",
    "            test_out = OutputBuffer(outputDir=out_sub_dir,\n",
    "                                    edf=0,\n",
    "                                    tif=True,\n",
    "                                    overwrite=True,\n",
    "                                    outputRoot=hdf_file, \n",
    "                                    fileEntry=hdf_file\n",
    "                                    )\n",
    "\n",
    "            FastXRFLinearFit.fitMultipleSpectra(pymca_object, \n",
    "                                                y=spectra_stack, \n",
    "                                                weight=weight, \n",
    "                                                refit=refit_state, \n",
    "                                                concentrations=False,\n",
    "                                                outbuffer=test_out)\n",
    "\n",
    "            # Carry over the fpico normalisation mask is exists\n",
    "            output_hdf_fpath = list(out_sub_dir.glob('*h5'))[0]\n",
    "\n",
    "            with h5py.File(input_hdf_fpath, 'r') as hdf:\n",
    "                if 'fpico_mask' in hdf:\n",
    "                    fpico_mask = hdf['fpico_mask'][:]\n",
    "                else:\n",
    "                    fpico_mask = False\n",
    "\n",
    "            print('output hdf file:', output_hdf_fpath)\n",
    "\n",
    "            if isinstance(fpico_mask, np.ndarray):\n",
    "                with h5py.File(output_hdf_fpath, 'r+') as hdf:\n",
    "                      hdf.create_dataset(name = 'fpico_mask', data = fpico_mask)\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform deconvolutions on both detectors (where present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " appendix_a1_overview_solid_0002\n",
      "1562500\n",
      "small dataset z 1250 y 1250\n",
      "spectra shape (1562500, 4096)\n",
      "reshaped spectra shape (1250, 1250, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\appendix_a1_overview_solid_0002\\falconx_det0\\appendix_a1_overview_solid_0002.h5\n",
      "\n",
      " appendix_a1_ROI_solid_0001\n",
      "960000\n",
      "small dataset z 800 y 1200\n",
      "spectra shape (960000, 4096)\n",
      "reshaped spectra shape (800, 1200, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\appendix_a1_ROI_solid_0001\\falconx_det0\\appendix_a1_ROI_solid_0001.h5\n",
      "\n",
      " tonsil_t1_overview_solid_0001\n",
      "1562500\n",
      "small dataset z 1250 y 1250\n",
      "spectra shape (1562500, 4096)\n",
      "reshaped spectra shape (1250, 1250, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\tonsil_t1_overview_solid_0001\\falconx_det0\\tonsil_t1_overview_solid_0001.h5\n",
      "\n",
      " tonsil_t1_ROI_solid_0001\n",
      "640000\n",
      "small dataset z 800 y 800\n",
      "spectra shape (640000, 4096)\n",
      "reshaped spectra shape (800, 800, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\tonsil_t1_ROI_solid_0001\\falconx_det0\\tonsil_t1_ROI_solid_0001.h5\n",
      "\n",
      " breast_cancer_b2_solid_overview_0corrected_0003\n",
      "6250000\n",
      "large dataset z 2500 y 2500\n",
      "Chunks: 10\n",
      "[[0, 625000], [625000, 1250000], [1250000, 1875000], [1875000, 2500000], [2500000, 3125000], [3125000, 3750000], [3750000, 4375000], [4375000, 5000000], [5000000, 5625000], [5625000, 6250000]]\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_0\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_1\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_2\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_3\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_4\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_5\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_6\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_7\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_8\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "spectra shape (625000, 4096)\n",
      "reshaped spectra shape (250, 2500, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\chunk_9\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "\n",
      " breast_cancer_b2_ROI_solid_ROI_0001\n",
      "22500\n",
      "small dataset z 150 y 150\n",
      "spectra shape (22500, 4096)\n",
      "reshaped spectra shape (150, 150, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_ROI_solid_ROI_0001\\falconx_det0\\breast_cancer_b2_ROI_solid_ROI_0001.h5\n",
      "\n",
      " breast_cancer_b2_ROI_solid_0003\n",
      "640000\n",
      "small dataset z 800 y 800\n",
      "spectra shape (640000, 4096)\n",
      "reshaped spectra shape (800, 800, 4096)\n",
      "spectra config file: 221217_snip23_esrf2022.cfg\n",
      "output hdf file: C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_ROI_solid_0003\\falconx_det0\\breast_cancer_b2_ROI_solid_0003.h5\n"
     ]
    }
   ],
   "source": [
    "for hdf_file in hdf_list:\n",
    "    print('\\n', hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state=True)\n",
    "#     xrf_deconvolute(hdf_file, 'fluodet_det0', refit_state=True)\n",
    "\n",
    "print('\\n All files processed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate chunked deconvolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer_b2_solid_overview_0corrected_0003\n",
      "chunk_1\n",
      "chunk_2\n",
      "chunk_3\n",
      "chunk_4\n",
      "chunk_5\n",
      "chunk_6\n",
      "chunk_7\n",
      "chunk_8\n",
      "chunk_9\n"
     ]
    }
   ],
   "source": [
    "# ID chunked deconvolutions\n",
    "chunk_subdirs = list(out_dir.glob('*/*/*/chunk*'))\n",
    "chunk_dirs = list(np.unique([i.parent for i in chunk_subdirs]))\n",
    "\n",
    "# Find first hdf and copy to expected location\n",
    "for chunk_dir in chunk_dirs:\n",
    "    print(chunk_dir.parent.stem)\n",
    "\n",
    "    # Identify chunks and concatenate\n",
    "    hdf_chunks = list(chunk_dir.glob('*/*.h5'))    \n",
    "    for i, hdf_chunk in enumerate(hdf_chunks[:]):\n",
    "        if i == 0:\n",
    "            hdf_base_path = chunk_dir / hdf_chunks[0].name\n",
    "            shutil.copyfile(hdf_chunks[0], hdf_base_path) \n",
    "        else:\n",
    "            print(hdf_chunk.parent.stem)\n",
    "            with h5py.File(hdf_base_path, 'a') as hdf_base:\n",
    "                with h5py.File(hdf_chunk, 'r') as hdf_add:\n",
    "                    node = f\"{hdf_base_path.stem}/plotselect\"\n",
    "                    for plot in hdf_base[node].keys():\n",
    "                        subnode = f\"{node}/{plot}\"\n",
    "                        array_top = hdf_base[subnode][:]\n",
    "                        array_base = hdf_add[subnode][:]\n",
    "                        array_stacked = np.concatenate([array_top, array_base], axis=0)\n",
    "                        \n",
    "                        del hdf_base[subnode]\n",
    "                        hdf_base.create_dataset(subnode, data=array_stacked)\n",
    "                        \n",
    "    # Extract tifs from hdf channels\n",
    "    with h5py.File(hdf_base_path, 'r+') as hdf:\n",
    "        image_dir = hdf_base_path.parent / \"IMAGES\"\n",
    "        image_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img in hdf[node].keys():\n",
    "            subnode = f\"{node}/{img}\"\n",
    "            dset = hdf[subnode][:]\n",
    "            outpath = str(image_dir / f'{img}.tiff')\n",
    "            cv2.imwrite(outpath, dset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Incorporate step sizes for final channel image hdf files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\appendix_a1_overview_solid_0002\\falconx_det0\\appendix_a1_overview_solid_0002.h5\n",
      "detector in hdf, overwriting\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\appendix_a1_ROI_solid_0001\\falconx_det0\\appendix_a1_ROI_solid_0001.h5\n",
      "detector in hdf, overwriting\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\tonsil_t1_overview_solid_0001\\falconx_det0\\tonsil_t1_overview_solid_0001.h5\n",
      "detector in hdf, overwriting\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\tonsil_t1_ROI_solid_0001\\falconx_det0\\tonsil_t1_ROI_solid_0001.h5\n",
      "detector in hdf, overwriting\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_solid_overview_0corrected_0003\\falconx_det0\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "detector not in hdf, creating\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_ROI_solid_ROI_0001\\falconx_det0\\breast_cancer_b2_ROI_solid_ROI_0001.h5\n",
      "detector in hdf, overwriting\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\per_detector_deconvolutions\\breast_cancer_b2_ROI_solid_0003\\falconx_det0\\breast_cancer_b2_ROI_solid_0003.h5\n",
      "detector in hdf, overwriting\n",
      "Key acquisition metadata incorporated to .hdf files\n"
     ]
    }
   ],
   "source": [
    "# Incorporate key csv attributes per scan to deconvoluted hdfs for subsequent analysis\n",
    "# hdf_img_fpaths = [i for i in out_dir.glob('*/*/*/*.h5')]\n",
    "hdf_img_fpaths = [f\"{hdf_file}.h5\" for hdf_file in df_hdf_config['hdf_file']]\n",
    "list(out_dir.glob(f\"*/*/*/{hdf_img_fpaths[0]}\"))[0]\n",
    "hdf_img_fpaths = [list(out_dir.glob(f\"*/*/*/{hdf_file}\"))[0] for hdf_file in hdf_img_fpaths]\n",
    "\n",
    "for hdf_img_fpath in hdf_img_fpaths:\n",
    "    print(hdf_img_fpath)\n",
    "    step = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'step_um'].iloc[0]\n",
    "    det_type = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'detector'].iloc[0]\n",
    "    dual_det = df_hdf_config.loc[df_hdf_config['hdf_file'] == hdf_img_fpath.stem, 'dual_detector'].iloc[0]\n",
    "    \n",
    "    if dual_det == 1:\n",
    "        dual_det = 'yes'\n",
    "    else:\n",
    "        dual_det = 'no'\n",
    "   \n",
    "    with h5py.File(hdf_img_fpath, 'a') as hdf:\n",
    "        if 'pixel_um' in hdf:\n",
    "            pass \n",
    "        else:\n",
    "            hdf.create_dataset(name = 'pixel_um', data = step)\n",
    "        if 'detector' in hdf:\n",
    "            print('detector in hdf, overwriting')\n",
    "            del hdf['detector']\n",
    "            hdf.create_dataset('detector', data=det_type)\n",
    "        else:\n",
    "            print('detector not in hdf, creating')\n",
    "            hdf.create_dataset(name = 'detector', data = det_type)\n",
    "        if 'dual_detector' in hdf:\n",
    "            pass \n",
    "        else:\n",
    "            hdf.create_dataset(name = 'dual_detector', data = dual_det)\n",
    "\n",
    "print('Key acquisition metadata incorporated to .hdf files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate hdfs to summary hdfs per sample. For scans with dual detectors, element plots will be aggregated, while for single detector scans these will simply be copied over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary hdfs output to:  C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\2_deconvoluted_hdfs\\summary_hdfs\n"
     ]
    }
   ],
   "source": [
    "detector_paths = ['falconx_det0', 'fluodet_det0']\n",
    "out_sum_dir = out_dir / 'summary_hdfs'\n",
    "out_sum_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the falconX detector to summary scan output\n",
    "for hdf_file in df_hdf_config['hdf_file']:\n",
    "    hdf_dir = out_dir / 'per_detector_deconvolutions' / hdf_file \n",
    "    hdf_fpath = list(hdf_dir.glob('*/*.h5'))\n",
    "    \n",
    "    shutil.copyfile(hdf_fpath[0], out_sum_dir / hdf_fpath[0].name)\n",
    "            \n",
    "# For dual detector scans, modify the summary scan datasets by aggregating second detector data for plots\n",
    "for hdf_file in df_hdf_config.loc[df_hdf_config['dual_detector'] == 1, 'hdf_file']:\n",
    "    hdf_dir = out_dir / 'per_detector_deconvolutions' / hdf_file \n",
    "    hdf_fpath = list(hdf_dir.glob('*/*.h5'))\n",
    "    \n",
    "    hdf_base = h5py.File(hdf_fpath[0], 'r+')\n",
    "    plots = list(hdf_base[f'{hdf_file}/plotselect'].keys())\n",
    "    \n",
    "    for plot in plots:\n",
    "        dset = hdf_base[f'{hdf_file}/plotselect/{plot}'][:]\n",
    "        \n",
    "        with h5py.File(hdf_fpath[1], 'r+') as hdf_add:\n",
    "            dset_add = hdf_add[f'{hdf_file}/plotselect/{plot}'][:]\n",
    "            \n",
    "        dset_mean = np.mean([dset, dset_add], axis=0)\n",
    "        \n",
    "        hdf_mod_path = out_sum_dir / hdf_fpath[0].name\n",
    "        #print(hdf_mod_path.exists())\n",
    "        with h5py.File(hdf_mod_path, 'r+') as hdf_mod:\n",
    "            hdf_mod[f'{hdf_file}/plotselect/{plot}'][...] = dset_mean\n",
    "        \n",
    "    #print(dset_mean.shape) \n",
    "    hdf_base.close()\n",
    "    \n",
    "print('Summary hdfs output to: ', out_sum_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\\n\\n# Refits of silicon drift detector cell pellets\\nhdf_sublist = ['001_002_stitch', '001_004_stitch',  'sample001_0001', 'sample001_0003']\\n\\nfor hdf_file in hdf_sublist:\\n    print(hdf_file)\\n    xrf_deconvolute(hdf_file, 'falconx_det0')\\n    xrf_deconvolute(hdf_file, 'fluodet_det0')\\n    \\nprint('deconvolutions complete')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\n",
    "\n",
    "# Refits of silicon drift detector cell pellets\n",
    "hdf_sublist = ['001_002_stitch', '001_004_stitch',  'sample001_0001', 'sample001_0003']\n",
    "\n",
    "for hdf_file in hdf_sublist:\n",
    "    print(hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0')\n",
    "    xrf_deconvolute(hdf_file, 'fluodet_det0')\n",
    "    \n",
    "print('deconvolutions complete')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\\n\\n# Refits of GeCMOS detector breast cancers\\nhdf_sublist = ['sample304_0001', 'sample304_0002', 'sample304_b_0001', 'sample304_b_0002', 'sample304_b_0003', 'sample304_b_0004']\\n\\nfor hdf_file in hdf_sublist:\\n    print(hdf_file)\\n    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state = True, weight=1)\\n    \\nprint('deconvolutions complete')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#TEMP CELL, CAN BE DELETED FOR FINAL RELEASE, JUST IF NEED TO REFIT CERTAIN FILES\n",
    "\n",
    "# Refits of GeCMOS detector breast cancers\n",
    "hdf_sublist = ['sample304_0001', 'sample304_0002', 'sample304_b_0001', 'sample304_b_0002', 'sample304_b_0003', 'sample304_b_0004']\n",
    "\n",
    "for hdf_file in hdf_sublist:\n",
    "    print(hdf_file)\n",
    "    xrf_deconvolute(hdf_file, 'falconx_det0', refit_state = True, weight=1)\n",
    "    \n",
    "print('deconvolutions complete')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
