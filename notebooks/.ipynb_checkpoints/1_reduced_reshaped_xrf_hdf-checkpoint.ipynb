{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing of XRF hdf files\n",
    "## Summary\n",
    "This notebook reduces the raw .hdf XRF files collected at the ID15A beamline (European Synchrotron Radiation Facility) to pre-processed hdf files. These pre-processed hdf files have a consistent dataset naming strategy and contain only the datasets required to produce deconvoluted 2D XRF data. Datasets in pre-processed datasets include:\n",
    "1. stage coordinates \n",
    "    + hrz (z position)\n",
    "    + hry (y position)\n",
    "2. spectra per pixel \n",
    "    + falconx_det0 (detector 1)\n",
    "    + fluodet_det0 (detector 2, used in some scans)\n",
    "3. beam flux measurements local to the sample environment collected at the time interval trigger (for normalisation of XRF spectra intensities)\n",
    "    + fpico3\n",
    "\n",
    "### Stitching\n",
    "As well as reducing hdf datasets to a consistent format, this notebook can stitch together fragmented scans (scans formed by multiple scans). For all scans then:\n",
    "+ Complete scans are reduced to the key datasets needed for 2D XRF mapping\n",
    "+ Incomplete scans that need to be stitched together are stitched then reduced to the key datasets needed for 2D XRF mapping.\n",
    "    This requires that:\n",
    "    1. Scans to be stitched have at least 1 row of scan overlap (the notebook will detect this via row coordinates stored in the `hrz` dataset of raw hdf files)\n",
    "    2. Scans to be stitched together are identified in `xrf_scan_metadata.csv` in the raw data directory. Within `xrf_scan_metadata.csv`, a `scanset` column indicates scans that need to be stitched together by a common name. \n",
    "\n",
    "All reduced .hdf files are output to `data/processed/xrf/1_reduced_reshaped`. \n",
    "\n",
    "Output reduced .hdf files are suitable for further processing, including flux normalisation (see notebook 2) or straight deconvolution (see notebook 3) to generate per channel images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up folder structure\n",
    "The following cell sets up the folder structure required. Only the base directory may need modifying. \n",
    "\n",
    "In the base directory, if a raw subdirectory does not exist it will be made. This raw subdirectory should contain the raw .hdf files of interest and `xrf_scan_metadata.csv`. Within `xrf_scan_metadata.csv`, a column listing the filepath to each raw .hdf file of interest is required. \n",
    "\n",
    "A processed subdirectory will be made for the stitched .hdf files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped hdf files output to: \t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\n"
     ]
    }
   ],
   "source": [
    "# Define base directory to work from \n",
    "base_dir = \"C:/Users/MerrickS/OneDrive/Work/2_UZH/Papers/1_MEZ_XRF\"\n",
    "base_dir = pathlib.Path(base_dir)\n",
    "\n",
    "# Make raw directory for raw hdf files if it does not exist\n",
    "input_dir = base_dir / 'data' / 'raw' / 'xrf'\n",
    "input_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Make output directory for reshaped hdf files if it does not exist\n",
    "out_dir = base_dir / 'data' / 'processed' / 'xrf' / '1_reduced_reshaped_hdfs'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Reshaped hdf files output to: \\t', out_dir)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select raw datasets to include in pre-processed hdf files\n",
    "Specify which datasets to copy from raw data to pre-processed hdf datasets. Datasets will be stored as dictionary keys to which values are ascribed prior to saving into the preprocessed hdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the hdf parameters to extract and incorpate into new stitched hdf file (can add\n",
    "# extras here if look at the keys in hdf)\n",
    "hdf_datasets = ['1.1/measurement/hrz',\n",
    "                '1.1/measurement/hry',\n",
    "                '1.1/measurement/fpico3',\n",
    "                '1.1/measurement/falconx_det0',\n",
    "                '1.1/measurement/fluodet_det0',\n",
    "                ]\n",
    "\n",
    "# Convert datasets list to a dictionary (no values at this point)\n",
    "hdf_datasets = dict.fromkeys(hdf_datasets)\n",
    "\n",
    "# Make a dataframe to track preprocessed scans and scansets with deconvolution configs\n",
    "preprocessed_files_cols = ['hdf_file', 'config_file', 'step_um', 'dual_detector']\n",
    "df_preprocessed_files = pd.DataFrame(columns = preprocessed_files_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify scans to process\n",
    "Import a .csv spreadsheet named `xrf_scan_metadata.csv` from the /data/raw/xrf/ subdirectory. This .csv list all scans of interest and identifies which scans need to be stitched together. \n",
    "\n",
    "`xrf_scan_metadata.csv` should include\n",
    "1. A `scan_name` column which will be used to name the pre-processed dataset\n",
    "2. A `hdf_filepath` column that points to the raw data location relative to the base directory.\n",
    "3. A `scanset` column that identifies scans to be stitched together as those scans with the same scanset name. The scanset name will be used to name the file\n",
    "\n",
    "The next cell collects filepaths, files, filenames to be processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Get .hdf scan names and paths from hdf_filepath supplied in raw metadata \n",
    "#   csv\n",
    "df_hdf_metadata = pd.read_csv(input_dir / 'xrf_scan_metadata2.csv')\n",
    "\n",
    "# Make a dataframe of paths, files and fnames from hdf_filepath to include in 'xrf_scan_metadata_full.csv'\n",
    "full_fpaths = []\n",
    "paths = []\n",
    "files = []\n",
    "fnames = []\n",
    "for hdf_filepath in list(df_hdf_metadata['hdf_filepath']):\n",
    "    hdf_filepath = pathlib.Path(hdf_filepath)\n",
    "    \n",
    "    fullpath = hdf_filepath\n",
    "        \n",
    "    full_fpaths.append(fullpath)\n",
    "    paths.append(hdf_filepath.parent)\n",
    "    files.append(hdf_filepath.name)\n",
    "    fnames.append(hdf_filepath.stem)\n",
    "\n",
    "df_hdf_metadata['hdf_full_fpaths'] = full_fpaths\n",
    "df_hdf_metadata['hdf_path'] = paths\n",
    "df_hdf_metadata['hdf_file'] = files\n",
    "df_hdf_metadata['hdf_filename'] = fnames\n",
    "\n",
    "df_hdf_metadata.to_csv((out_dir / 'xrf_scan_metadata_full2.csv'), index=False)\n",
    "\n",
    "# check hdf files exist\n",
    "for filepath in full_fpaths:\n",
    "    if filepath.exists() == True:\n",
    "        pass\n",
    "    else:\n",
    "        print (\"\\t !!!File does not exist!!!:\", fullpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next cell identifies complete scans and scansets from the imported csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 8 complete scans identified for reduction:\n",
      "\t appendix_a1_overview_solid_0001\n",
      "\t appendix_a1_overview_solid_0002\n",
      "\t appendix_a1_ROI_solid_0001\n",
      "\t tonsil_t1_overview_solid_0001\n",
      "\t tonsil_t1_ROI_solid_0001\n",
      "\t breast_cancer_b2_solid_overview_0corrected_0003\n",
      "\t breast_cancer_b2_ROI_solid_ROI_0001\n",
      "\t breast_cancer_b2_ROI_solid_0003\n",
      "\n",
      " 0 scansets identified for stitching and reduction from metadata:\n",
      "\n",
      " Scan names for scansets listed in 'scansets' dictionary\n"
     ]
    }
   ],
   "source": [
    "# Get complete scans\n",
    "df_complete_scans = df_hdf_metadata[df_hdf_metadata['scanset'].isnull()]\n",
    "complete_scans = df_complete_scans['hdf_filename'].unique().tolist()\n",
    "print('\\n', len(complete_scans), 'complete scans identified for reduction:')    \n",
    "for scan in complete_scans: print('\\t', scan)\n",
    "\n",
    "# Get incomplete scans that are part of a scanset to stitch\n",
    "df_scansets = df_hdf_metadata[df_hdf_metadata['scanset'].notnull()]\n",
    "scansets = df_scansets['scanset'].unique().tolist() # scansets unique names\n",
    "\n",
    "print('\\n', len(scansets), 'scansets identified for stitching and reduction from metadata:')    \n",
    "for scanset in scansets: print('\\t',scanset)\n",
    "\n",
    "hdfs_to_stitch = [] # list of scans for each scanset\n",
    "for scanset in scansets:\n",
    "    scanset_scans = df_scansets.hdf_filename[df_scansets['scanset'] == scanset].to_list()\n",
    "    hdfs_to_stitch.append(scanset_scans)\n",
    "    \n",
    "# Turn scansets to a dictionary for each scanset (keys) of .hdfs (values) \n",
    "# that need stitching\n",
    "scansets = dict(zip(scansets, hdfs_to_stitch)) \n",
    "\n",
    "print('\\n Scan names for scansets listed in \\'scansets\\' dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hdf dataset crop functions\n",
    "Load functions needed to define crop dimensions for scansets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% This function returns the hrz (number of rows and their coordinates) \n",
    "#   for a specified hdf file\n",
    "def hdf_hrz_list(hdf_filepath): \n",
    "    with h5py.File(hdf_filepath, 'r') as hdf:\n",
    "        hrz_list = list(hdf['1.1/measurement/hrz'])\n",
    "    return hrz_list\n",
    "\n",
    "#%% This function extracts the named dataset from an hdf file as an hdf dataset\n",
    "def hdf_dataset(hdf_filepath, dataset): \n",
    "    with h5py.File(hdf_filepath, 'r') as hdf:\n",
    "        node = f\"{dataset}\"\n",
    "        try:\n",
    "            hdf[node]\n",
    "            #print(f'{node} exists')       \n",
    "            dset = hdf[f'{dataset}']\n",
    "        except KeyError:\n",
    "            print(f'\\t !!! {node} does not exist, could not retrieve dataset')\n",
    "            dset = []\n",
    "        dset = dset[:]\n",
    "    return dset\n",
    "\n",
    "#%% This function identifies the  overlapping rows in a list of hdf5 \n",
    "#   consecutive scans. It returns a list of lengths by which to crop data \n",
    "#   within overlapping scans. This crop_list can be used to slice overlapping\n",
    "#   scans so they are suitable to stitch together.\n",
    "def hdf_list_crop(hdf_filepaths):  \n",
    "    scan_number = len(hdf_filepaths)\n",
    "    \n",
    "    # Make a list of hrz position lists\n",
    "    z_pos = []\n",
    "    y_cols = []\n",
    "    for scan in hdf_filepaths:\n",
    "        scan_filepath = base_dir / scan\n",
    "        \n",
    "        with h5py.File(scan_filepath, 'r') as hdf_initialise:\n",
    "            hdf_z_pos = hdf_hrz_list(scan_filepath)\n",
    "\n",
    "            # in some .hdfs, the fscan parameters are not recorded, so column number must be calculated\n",
    "            node = '1.1/instrument/fscan_parameters/fast_npoints'              \n",
    "            try:\n",
    "                ycols_in_scan = hdf_initialise[node][()]\n",
    "            except KeyError:\n",
    "                ycols_in_scan = hdf_z_pos.count(hdf_z_pos[0]) # get number of columns in a row, by counting number of z_pos in first row       \n",
    "\n",
    "        z_pos.append(hdf_z_pos)\n",
    "        y_cols.append(ycols_in_scan) # get number of columns in a row, by counting number of z_pos in first row\n",
    "        print(f'\\t Scan: {scan} \\n\\t has {len(set(hdf_z_pos))} rows & {y_cols[-1]} columns')\n",
    "        \n",
    "    # Check scans have the same number of columns that can be stitched\n",
    "    if y_cols.count(y_cols[0]) == len(y_cols):\n",
    "        y_cols = y_cols[0]\n",
    "        print(f'\\t Same number of columns in each scan so scanset {scanset} suitable for stitching',\n",
    "              f'\\n\\t Proceeding to determine crop dimensions for {scanset}')\n",
    "    else:\n",
    "        print(f'\\t Different number of columns in each scan',\n",
    "              f'\\n\\t Scanset {scanset} cannot be stitched')\n",
    "    \n",
    "    # Get hrz rows overlap for successive scans\n",
    "    z_overlap_rows = []\n",
    "    for scan in range(scan_number-1): #check overlap between scan pairs, except last scan\n",
    "        z_overlap = list((set(z_pos[scan])).intersection(set(z_pos[scan+1])))\n",
    "        z_overlap_rows.append(len(z_overlap))\n",
    "        print(f'\\t Scan {scan+1} overlaps with scan {scan+2} by {z_overlap_rows[-1]} rows.')\n",
    "        print(f'\\t Last {z_overlap_rows[-1]} rows will be dropped from scan {scan+1} in crop list')\n",
    "    z_overlap_rows.append(0)\n",
    "        \n",
    "    # Length of datapoints to keep from each hdf file\n",
    "    crop_list = []\n",
    "    for scan, value in enumerate(z_pos):\n",
    "        rows = len(set(value))\n",
    "        crop_list.append((rows-z_overlap_rows[scan])*y_cols)\n",
    "                \n",
    "    return crop_list\n",
    "\n",
    "def scannames_to_filepaths(scans):\n",
    "    filepaths = []\n",
    "    for scan in scans:\n",
    "        filepath = df_hdf_metadata.loc[df_hdf_metadata['hdf_filename'] == scan, 'hdf_full_fpaths'].iloc[0]\n",
    "        filepaths.append(filepath)\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get scanset crop dimensions\n",
    "For identified scansets, define how to crop scans within a scanset to stitch together. \n",
    "\n",
    "Scanset crop dimensions will be stored as a `crop_dict` dictionary, with scanset name as key and crop dimensions as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Crop dimensions for scansets\n",
      "\n",
      "\n",
      " All scanset crop dimensions identified\n"
     ]
    }
   ],
   "source": [
    "crop_dict = {}\n",
    "for scanset in scansets:\n",
    "    print(f'Scanset {scanset}:')\n",
    "    hdf_filepaths = scannames_to_filepaths(scansets[scanset])\n",
    "    crop_dict.update({scanset:hdf_list_crop(hdf_filepaths)})\n",
    "           \n",
    "print('\\n\\n Crop dimensions for scansets')\n",
    "for scanset in scansets:\n",
    "    print('\\t', scanset, 'crop dimensions', crop_dict[scanset])\n",
    "\n",
    "print('\\n\\n All scanset crop dimensions identified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch scansets to pre-processed hdf files\n",
    "Using the crop dimensions for scans in scansets stored in `crop_dict`, we can now stitch scansets together to the pre-processed datasets.\n",
    "\n",
    "The following cell first loads the necessary functions for stitching list or array hdf datasets, and the metafunction `stitch_hdf_array_or_list_dset` used by `scanset_reduced_hdf` to produce the new reshaped and reduced datasets. \n",
    "\n",
    "Reduced stitched scans are output to the `data/processed/xrf/1_reduced_reshaped_hdfs` directory and identified with the `_stitch` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All scansets finished stitching\n"
     ]
    }
   ],
   "source": [
    "#%% This function return a stitched list dataset for specified list datsets in \n",
    "#   an hdf5. It requires 1) a list of hdf files containing the specified \n",
    "#   dataset, 2) a list of lengths to crop each dataset and 3) the dataset name\n",
    "def stitch_hdf_list_dset(hdf_list, crop_dims, dset): # (list, list, 'string')\n",
    "    data = []\n",
    "    for number, scan in enumerate(hdf_list):\n",
    "        with h5py.File(scan, 'r') as hdf:\n",
    "            data_add = hdf[f'{dset}'][:]\n",
    "            data_add = data_add[:crop_dims[number]]\n",
    "            data.extend(data_add)\n",
    "    return data\n",
    "\n",
    "def stitch_hdf_array_dset(hdf_list, crop_dims, dset): \n",
    "    for number, scan in enumerate(hdf_list):\n",
    "        if number == 0:\n",
    "            with h5py.File(scan, 'r') as hdf:\n",
    "                data = hdf[f'{dset}'][:]\n",
    "                data = data[:crop_dims[number]]\n",
    "        else:    \n",
    "            with h5py.File(scan, 'r') as hdf:\n",
    "                data_add = hdf[f'{dset}'][:]\n",
    "                data_add = data_add[:crop_dims[number]]\n",
    "                data = np.concatenate((data, data_add), axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "#%% This function identifies if the dataset to stitch is a list or an array.\n",
    "#   It then selects the appropriate function to stitch the dataset type\n",
    "def stitch_hdf_array_or_list_dset(hdf_list, crop_dims, dset):\n",
    "    print(hdf_list[0])\n",
    "    with h5py.File(hdf_list[0], 'r') as hdf_initialise:\n",
    "        shape = hdf_initialise[f'{dset}'].shape\n",
    "        shape = (0, shape[-1])\n",
    "    if len(shape) == 1:\n",
    "        print('list data')\n",
    "        data = stitch_hdf_list_dset(hdf_list, crop_dims, dset)\n",
    "    else:\n",
    "        print('array data')\n",
    "        data = stitch_hdf_array_dset(hdf_list, crop_dims, dset)        \n",
    "    return data\n",
    "   \n",
    "def scanset_reduced_hdf(scanset, datasets):\n",
    "    \"\"\"\n",
    "    This function creates a new hdf file for acqusitions comprising \n",
    "    multiple scans that need stitching.\n",
    "    \"\"\"\n",
    "    hdf_list = scannames_to_filepaths(scansets[scanset])\n",
    "        \n",
    "    print('Scans to stitch:')\n",
    "    for scan in hdf_list:\n",
    "        print('\\t', scan)\n",
    "    crop_dims = crop_dict[scanset]\n",
    "    print('Crop dimensions:', crop_dims)\n",
    "    \n",
    "    hdf_new_name = base_dir / out_dir / f'{scanset}.h5'\n",
    "    print(hdf_new_name)\n",
    "    \n",
    "    with h5py.File(hdf_new_name, 'w') as hdf_new:     \n",
    "        for dset in datasets:\n",
    "            print(f'\\t Adding {scanset} {dset} dataset to new hdf file')\n",
    "            hdf_datasets[dset] = stitch_hdf_array_or_list_dset(hdf_list = hdf_list,\n",
    "                                                               crop_dims = crop_dims, \n",
    "                                                               dset = dset)\n",
    "            dset_name = dset.rpartition('/')[-1]\n",
    "                \n",
    "            hdf_new.create_dataset(dset_name, data=hdf_datasets[dset], compression='gzip')\n",
    "        \n",
    "    print(f'\\t Preprocessed hdf file for scanset {scanset} output to: \\n\\t {hdf_new_name}')\n",
    "    \n",
    "    \n",
    "# Stitch scansets together and output the new hdf files\n",
    "for scanset in scansets:\n",
    "    print(f'\\n Stitching scanset:', scanset)\n",
    "    scanset_reduced_hdf(scanset = scanset, datasets = hdf_datasets)\n",
    "    \n",
    "    config_file = df_scansets.loc[df_scansets['scanset'] == scanset, 'config_file'].iloc[0]\n",
    "    step_um = df_scansets.loc[df_scansets['scanset'] == scanset, 'step_um'].iloc[0]\n",
    "    dual_detector = df_scansets.loc[df_scansets['scanset'] == scanset, 'dual_detector'].iloc[0]\n",
    "    det_type = df_scansets.loc[df_scansets['scanset'] == scanset, 'detector'].iloc[0]\n",
    "   \n",
    "    df_preprocessed_files = df_preprocessed_files.append({'config_file':config_file, \n",
    "                                                          'hdf_file':scanset, \n",
    "                                                          'step_um':step_um,\n",
    "                                                          'dual_detector':dual_detector,\n",
    "                                                          'detector':det_type\n",
    "                                                         }, ignore_index=True)\n",
    "    \n",
    "print('\\n All scansets finished stitching')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce complete scans to pre-processed hdf files\n",
    "For complete scans that do not need to be stitched, the following cell first loads the metafunction `scan_reduced_hdf` to produce the reduced pre-processed dataset.\n",
    "\n",
    "Pre-processed datasets are output to the `data/processed/xrf/1_reduced_reshaped_hdfs` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessing scan: appendix_a1_overview_solid_0001\n",
      "\t Adding appendix_a1_overview_solid_0001 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0001 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0001 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0001 1.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0001 1.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 1.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan appendix_a1_overview_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: appendix_a1_overview_solid_0002\n",
      "\t Adding appendix_a1_overview_solid_0002 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0002 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0002 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0002 1.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding appendix_a1_overview_solid_0002 1.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 1.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan appendix_a1_overview_solid_0002 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0002.h5\n",
      "\n",
      " Preprocessing scan: appendix_a1_ROI_solid_0001\n",
      "\t Adding appendix_a1_ROI_solid_0001 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding appendix_a1_ROI_solid_0001 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding appendix_a1_ROI_solid_0001 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding appendix_a1_ROI_solid_0001 1.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding appendix_a1_ROI_solid_0001 1.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 1.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan appendix_a1_ROI_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_ROI_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: tonsil_t1_overview_solid_0001\n",
      "\t Adding tonsil_t1_overview_solid_0001 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding tonsil_t1_overview_solid_0001 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding tonsil_t1_overview_solid_0001 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding tonsil_t1_overview_solid_0001 1.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding tonsil_t1_overview_solid_0001 1.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 1.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan tonsil_t1_overview_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_overview_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: tonsil_t1_ROI_solid_0001\n",
      "\t Adding tonsil_t1_ROI_solid_0001 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding tonsil_t1_ROI_solid_0001 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding tonsil_t1_ROI_solid_0001 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding tonsil_t1_ROI_solid_0001 1.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding tonsil_t1_ROI_solid_0001 1.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 1.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan tonsil_t1_ROI_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_ROI_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: breast_cancer_b2_solid_overview_0corrected_0003\n",
      "\t Adding breast_cancer_b2_solid_overview_0corrected_0003 1.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding breast_cancer_b2_solid_overview_0corrected_0003 1.1/measurement/hry dataset to new hdf file\n",
      "\t Adding breast_cancer_b2_solid_overview_0corrected_0003 1.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding breast_cancer_b2_solid_overview_0corrected_0003 1.1/measurement/falconx_det0 dataset to new hdf file\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 95.4 GiB for an array with shape (6250000, 4096) and data type uint32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-02eae958903c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscan\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomplete_scans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Preprocessing scan:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mscan_reduced_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhdf_datasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mconfig_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_complete_scans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_complete_scans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hdf_filename'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-02eae958903c>\u001b[0m in \u001b[0;36mscan_reduced_hdf\u001b[1;34m(scan, datasets)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\t Adding {scan} {dset} dataset to new hdf file'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mhdf_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mdset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mhdf_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhdf_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-c2f34fb812e9>\u001b[0m in \u001b[0;36mhdf_dataset\u001b[1;34m(hdf_filepath, dataset)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\t !!! {node} does not exist, could not retrieve dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MerrickS\\Anaconda3\\envs\\mez-xrf\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0msingle_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[0mmshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msingle_element\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;31m# HDF5 has a bug where if the memory shape has a different rank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 95.4 GiB for an array with shape (6250000, 4096) and data type uint32"
     ]
    }
   ],
   "source": [
    "def scan_reduced_hdf(scan, datasets):\n",
    "    \"\"\"\n",
    "    This function creates a new hdf file for acqusitions comprising a single scan.\n",
    "    \"\"\"\n",
    "    hdf_filepath = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'hdf_full_fpaths'].iloc[0]\n",
    "       \n",
    "    hdf_new_name = base_dir / out_dir / f'{scan}.h5'\n",
    "    with h5py.File(hdf_new_name, 'w') as hdf_new:\n",
    "        for dset in datasets:\n",
    "            print(f'\\t Adding {scan} {dset} dataset to new hdf file')\n",
    "            hdf_datasets[dset] = hdf_dataset(hdf_filepath = hdf_filepath, dataset = dset)                                         \n",
    "            dset_name = dset.rpartition('/')[-1]\n",
    "            hdf_new.create_dataset(dset_name, data=hdf_datasets[dset], compression='gzip')       \n",
    "    print(f'\\t Preprocessed hdf file for scan {scan} output to: \\n\\t {hdf_new_name}')\n",
    "    \n",
    "for scan in complete_scans:\n",
    "    print('\\n Preprocessing scan:', scan)\n",
    "    scan_reduced_hdf(scan = scan, datasets=hdf_datasets)\n",
    "                \n",
    "    config_file = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'config_file'].iloc[0]\n",
    "    step_um = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'step_um'].iloc[0]\n",
    "    dual_detector = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'dual_detector'].iloc[0]\n",
    "    det_type = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'detector'].iloc[0]\n",
    "\n",
    "    df_preprocessed_files = df_preprocessed_files.append({'config_file':config_file, \n",
    "                                                          'hdf_file':scan, \n",
    "                                                          'step_um':step_um,\n",
    "                                                          'dual_detector':dual_detector,\n",
    "                                                          'detector':det_type\n",
    "                                                         }, ignore_index=True)\n",
    "       \n",
    "print('\\n All scans finished reducing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessing scan: appendix_a1_overview_solid_0001\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\appendix_a1_overview_solid\\appendix_a1_overview_solid_0001\\appendix_a1_overview_solid_0001.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0001.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan appendix_a1_overview_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: appendix_a1_overview_solid_0002\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\appendix_a1_overview_solid\\appendix_a1_overview_solid_0002\\appendix_a1_overview_solid_0002.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0002.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan appendix_a1_overview_solid_0002 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_overview_solid_0002.h5\n",
      "\n",
      " Preprocessing scan: appendix_a1_ROI_solid_0001\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\appendix_a1_ROI_solid\\appendix_a1_ROI_solid_0001\\appendix_a1_ROI_solid_0001.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_ROI_solid_0001.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan appendix_a1_ROI_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\appendix_a1_ROI_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: tonsil_t1_overview_solid_0001\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\tonsil_t1_overview_solid\\tonsil_t1_overview_solid_0001\\tonsil_t1_overview_solid_0001.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_overview_solid_0001.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan tonsil_t1_overview_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_overview_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: tonsil_t1_ROI_solid_0001\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\tonsil_t1_ROI_solid\\tonsil_t1_ROI_solid_0001\\tonsil_t1_ROI_solid_0001.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_ROI_solid_0001.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan tonsil_t1_ROI_solid_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\tonsil_t1_ROI_solid_0001.h5\n",
      "\n",
      " Preprocessing scan: breast_cancer_b2_solid_overview_0corrected_0003\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\breast_cancer_b2_solid_overview_0corrected\\breast_cancer_b2_solid_overview_0corrected_0003\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan breast_cancer_b2_solid_overview_0corrected_0003 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_solid_overview_0corrected_0003.h5\n",
      "\n",
      " Preprocessing scan: breast_cancer_b2_ROI_solid_ROI_0001\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\breast_cancer_b2_ROI_solid_ROI\\breast_cancer_b2_ROI_solid_ROI_0001\\breast_cancer_b2_ROI_solid_ROI_0001.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_ROI_solid_ROI_0001.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan breast_cancer_b2_ROI_solid_ROI_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_ROI_solid_ROI_0001.h5\n",
      "\n",
      " Preprocessing scan: breast_cancer_b2_ROI_solid_0003\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\raw\\xrf\\scans\\breast_cancer_b2_ROI_solid\\breast_cancer_b2_ROI_solid_0003\\breast_cancer_b2_ROI_solid_0003.h5\n",
      "C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_ROI_solid_0003.h5\n",
      "dataset 1.1/measurement/hrz\n",
      "1.1/measurement/hrz exists\n",
      "dataset 1.1/measurement/hry\n",
      "1.1/measurement/hry exists\n",
      "dataset 1.1/measurement/fpico3\n",
      "1.1/measurement/fpico3 exists\n",
      "dataset 1.1/measurement/falconx_det0\n",
      "1.1/measurement/falconx_det0 exists\n",
      "dataset 1.1/measurement/fluodet_det0\n",
      "\t Preprocessed hdf file for scan breast_cancer_b2_ROI_solid_0003 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\breast_cancer_b2_ROI_solid_0003.h5\n",
      "\n",
      " All scans finished reducing\n"
     ]
    }
   ],
   "source": [
    "def scan_reduced_hdf_large(scan, datasets):\n",
    "    \"\"\"\n",
    "    This function creates a new hdf file for acqusitions comprising a single scan.\n",
    "    \"\"\"\n",
    "    hdf_filepath = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'hdf_full_fpaths'].iloc[0]\n",
    "    print(hdf_filepath)\n",
    "       \n",
    "    hdf_new_name = base_dir / out_dir / f'{scan}.h5'\n",
    "    print(hdf_new_name)\n",
    "    \n",
    "    datasets = list(hdf_datasets.keys())\n",
    "        \n",
    "    with h5py.File(hdf_filepath,'r') as f_src:\n",
    "        with h5py.File(hdf_new_name,'w') as f_dest:\n",
    "\n",
    "            for dset in datasets:\n",
    "                print(\"dataset\", dset)\n",
    "\n",
    "                # Check dataset exists\n",
    "                e = dset in f_src\n",
    "\n",
    "                # Copy existing datasets\n",
    "                if e == True:\n",
    "                    print (dset, \"exists\")\n",
    "                    new_node = dset.split('/')[-1]\n",
    "                    f_src.copy(f_src[dset],f_dest, new_node) # copy to new hdf                \n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    print(f'\\t Preprocessed hdf file for scan {scan} output to: \\n\\t {hdf_new_name}')\n",
    "\n",
    "for scan in complete_scans:\n",
    "    print('\\n Preprocessing scan:', scan)\n",
    "    scan_reduced_hdf_large(scan = scan, datasets=hdf_datasets)\n",
    "                \n",
    "    config_file = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'config_file'].iloc[0]\n",
    "    step_um = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'step_um'].iloc[0]\n",
    "    dual_detector = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'dual_detector'].iloc[0]\n",
    "    det_type = df_complete_scans.loc[df_complete_scans['hdf_filename'] == scan, 'detector'].iloc[0]\n",
    "\n",
    "    df_preprocessed_files = df_preprocessed_files.append({'config_file':config_file, \n",
    "                                                          'hdf_file':scan, \n",
    "                                                          'step_um':step_um,\n",
    "                                                          'dual_detector':dual_detector,\n",
    "                                                          'detector':det_type\n",
    "                                                         }, ignore_index=True)\n",
    "       \n",
    "print('\\n All scans finished reducing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scans with different dataset structure\n",
    "During acquisition, some raw .hdf scans end up storing datasets in 2.1/measurements hdf node rather than 1.1/measurements. For these scans (which can be identified by small file size if reduced in the prior step), hdf reduction needs to be pointed to this different dataset structure. \n",
    "\n",
    "For scans listed in `scans_in_2pt1`, reduced .hdfs are rederived in the following cell by defining this structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessing scan: sample107_t_0001\n",
      "\t Adding sample107_t_0001 2.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding sample107_t_0001 2.1/measurement/hry dataset to new hdf file\n",
      "\t Adding sample107_t_0001 2.1/measurement/fpico3 dataset to new hdf file\n",
      "\t !!! 2.1/measurement/fpico3 does not exist, could not retrieve dataset\n",
      "\t Adding sample107_t_0001 2.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding sample107_t_0001 2.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t Preprocessed hdf file for scan sample107_t_0001 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\sample107_t_0001.h5\n",
      "\n",
      " Preprocessing scan: sample304_0017\n",
      "\t Adding sample304_0017 2.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding sample304_0017 2.1/measurement/hry dataset to new hdf file\n",
      "\t Adding sample304_0017 2.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding sample304_0017 2.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding sample304_0017 2.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t Preprocessed hdf file for scan sample304_0017 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\sample304_0017.h5\n",
      "\n",
      " Preprocessing scan: sample304_b_0004\n",
      "\t Adding sample304_b_0004 2.1/measurement/hrz dataset to new hdf file\n",
      "\t Adding sample304_b_0004 2.1/measurement/hry dataset to new hdf file\n",
      "\t Adding sample304_b_0004 2.1/measurement/fpico3 dataset to new hdf file\n",
      "\t Adding sample304_b_0004 2.1/measurement/falconx_det0 dataset to new hdf file\n",
      "\t Adding sample304_b_0004 2.1/measurement/fluodet_det0 dataset to new hdf file\n",
      "\t !!! 2.1/measurement/fluodet_det0 does not exist, could not retrieve dataset\n",
      "\t Preprocessed hdf file for scan sample304_b_0004 output to: \n",
      "\t C:\\Users\\MerrickS\\OneDrive\\Work\\2_UZH\\Papers\\1_MEZ_XRF\\data\\processed\\xrf\\1_reduced_reshaped_hdfs\\sample304_b_0004.h5\n",
      "\n",
      " Irregular complete scans processed to pre-processed directory\n"
     ]
    }
   ],
   "source": [
    "scans_in_2pt1 = ['sample107_t_0001', 'sample304_0017', 'sample304_b_0004']\n",
    "\n",
    "# Switch dataset directory from 1.1 to 2.1\n",
    "hdf_datasets_2 = list(hdf_datasets)\n",
    "hdf_datasets_2 = [dset.replace('1.1', '2.1') for dset in hdf_datasets_2]\n",
    "hdf_datasets_2 = dict.fromkeys(hdf_datasets_2)\n",
    "\n",
    "for scan in scans_in_2pt1:\n",
    "    print('\\n Preprocessing scan:', scan)\n",
    "    scan_reduced_hdf(scan = scan, datasets=hdf_datasets_2)\n",
    "    \n",
    "print('\\n Irregular complete scans processed to pre-processed directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export csv tracking preprocessed files alongside infomration key to deconvolution and later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_files.to_csv((out_dir / 'preprocessed_hdf_config_files2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fpico mask to normalise later plots according to X-ray flux. These will be added to the output hdf reduced, reshaped .h5 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 219352 into shape (176,1246)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-cbb170ec8ac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mhdf_img_fpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhdf_img_fpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mfpico_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf_fpico_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_img_fpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-87-cbb170ec8ac5>\u001b[0m in \u001b[0;36mhdf_fpico_mask\u001b[1;34m(hdf_img_fpath)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpico3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mfpico_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpico3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'fpico_mask'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 219352 into shape (176,1246)"
     ]
    }
   ],
   "source": [
    "# hdf_img_fpaths = [i for i in out_dir.glob('*.h5')]\n",
    "hdf_img_fpaths = [list(out_dir.glob(f'{i}.h5'))[0] for i in df_preprocessed_files['hdf_file']]\n",
    "\n",
    "def hdf_fpico_mask(hdf_img_fpath):\n",
    "    with h5py.File(hdf_img_fpath, 'r+') as hdf:    \n",
    "        fpico3 = hdf['fpico3'][:]\n",
    "        z = list(hdf['hrz'][:])\n",
    "        y = list(hdf['hry'][:])\n",
    "        \n",
    "        rows = len(np.unique(z))\n",
    "        cols = int(len(y)/rows)\n",
    "        \n",
    "        if len(fpico3) > 10:\n",
    "            fpico_mask = fpico3.reshape((rows, cols))\n",
    "            \n",
    "            if 'fpico_mask' in hdf:\n",
    "                hdf['fpico_mask'][...] = fpico_mask\n",
    "            else:\n",
    "                hdf.create_dataset(name = 'fpico_mask', data = fpico_mask)\n",
    "\n",
    "            return fpico_mask\n",
    "    \n",
    "for hdf_img_fpath in hdf_img_fpaths:\n",
    "    fpico_mask = hdf_fpico_mask(hdf_img_fpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in scan step size to each preprocessed hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf_img_fpaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-6fb08f8811f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mhdf_img_fpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhdf_img_fpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_preprocessed_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_preprocessed_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hdf_file'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mhdf_img_fpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'step_um'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf_img_fpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mhdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'step_um'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hdf_img_fpaths' is not defined"
     ]
    }
   ],
   "source": [
    "for hdf_img_fpath in hdf_img_fpaths:\n",
    "    step = df_preprocessed_files.loc[df_preprocessed_files['hdf_file'] == hdf_img_fpath.stem, 'step_um'].iloc[0]\n",
    "    \n",
    "    with h5py.File(hdf_img_fpath, 'r+') as hdf:\n",
    "        hdf.create_dataset(name = 'step_um', data = step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
